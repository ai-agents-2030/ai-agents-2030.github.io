<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DISTRL: AN ASYNCHRONOUS DISTRIBUTED REINFORCEMENT LEARNING FRAMEWORK FOR ON-DEVICE CONTROL AGENTS</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 2.4rem;">DISTRL: AN ASYNCHRONOUS DISTRIBUTED REINFORCEMENT LEARNING FRAMEWORK FOR ON-DEVICE CONTROL AGENTS</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Taiyi Wang<sup>1 2 * †</sup>,
              </span>
              <span class="author-block">
                Zhihao Wu<sup>3 *</sup>,
              </span>
              <span class="author-block">
                Jianheng Liu<sup>4</sup>,
              </span>
              <span class="author-block">
                Jianye Hao<sup>3</sup>,
              </span>
              <span class="author-block">
                Jun Wang<sup>4</sup>,
              </span>
              <span class="author-block">
                Kun Shao<sup>3 †</sup>
              </span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> University of Cambridge<br><sup>2</sup> Powersense Technology Limited<br><sup>3</sup> Huawei Noah's Ark Lab</span><br><sup>4</sup> University College London</span>
                    <span class="eql-cntrb"><small><br><sup>* </sup>Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>† </sup>Corresponding authors: Taiyi.Wang@cl.cam.ac.uk, shaokun2@huawei.com</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2410.14803" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/DistRL-lab/distrl-open" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.14803" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <!-- <h2 class="title is-3 has-text-centered">Overview</h2> -->
          <div class="content">
              <h3 class="subtitle is-5">Demo:</h3>
              <div style="text-align: center">
                <video poster="" id="tree" autoplay controls muted loop height="100%" width="75%">
                  <!-- Your video here -->
                  <source src="static/videos/compare_demo_v4.mp4" type="video/mp4">
                </video>
                <video poster="" id="tree" autoplay controls muted loop height="100%" width="75%" style="margin-top: 1rem">
                  <!-- Your video here -->
                  <source src="static/videos/async_demo_v5_compressed.mp4" type="video/mp4">
                </video>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            On-device control agents, especially on mobile devices, are responsible for operating mobile devices to fulfill users' requests, enabling seamless and intuitive interactions. Integrating Multimodal Large Language Models (MLLMs) into these agents enhances their ability to understand and execute complex commands, thereby improving user experience. However, fine-tuning MLLMs for on-device control presents significant challenges due to limited data availability and inefficient online training processes. This paper introduces DistRL, a novel framework designed to enhance the efficiency of online RL fine-tuning for mobile device control agents. DistRL employs centralized training and decentralized data acquisition to ensure efficient fine-tuning in the context of dynamic online interactions. Additionally, the framework is backed by our tailor-made RL algorithm, which effectively balances exploration with the prioritized utilization of collected data to ensure stable and robust training. Our experiments show that, on average, DistRL delivers a 3X improvement in training efficiency and enables training data collection 2.4X faster than the leading synchronous multi-machine methods. Notably, after training, DistRL achieves a 20% relative improvement in success rate compared to state-of-the-art methods on general Android tasks from an open benchmark, significantly outperforming existing approaches while maintaining the same training time. These results validate DistRL as a scalable and efficient solution, offering substantial improvements in both training efficiency and agent performance for real-world, in-the-wild device control tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
 
<!-- Plug-and-Play Agent Framework -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">A-RIDE: The Backbone of DistRL</h2>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <p style="text-align: justify;">Our method employs advantage-based estimations to refine policy gradient updates, as an extension of Generalized Advantage Estimation (GAE) by Schulman et al. (2015), to better suit asynchronous, distributed environments common in device control tasks.</p>

            <h4>Advantage Computation</h4>

            <p style="text-align: justify;">With the trajectory-level rewards and state value estimates obtained, we compute the advantage function A(s<sub>t</sub>, a<sub>t</sub>) using one-step estimation:</p>
            
            <p style="text-align :center; font-size: 1.25rem; font-family: math">A(s<sub>t</sub>, a<sub>t</sub>) = Q(s<sub>t</sub>,a<sub>t</sub>) - V(s<sub>t</sub>) = r(s<sub>t</sub>, a<sub>t</sub>) + γV(s<sub>t+1</sub>) - V(s<sub>t</sub>)</span></p>
            
            <p>which correctly represents the advantage function as per the policy gradient theorem. Here, r(s<sub>t</sub>, a<sub>t</sub>) includes signals of immediate rewards. The advantage and value functions are further modified by off-policy corrections.</p>
            
            <h4>Policy Optimization with Robust Regularization</h4>
            
            <p>Finally, the policy network optimizes the following loss:</p>
            
            <p style="text-align :center; font-size: 1.25rem; font-family: math">ℒ = −𝔼<sub>μ</sub>[ρ<sub>t</sub> A(s<sub>t</sub>, a<sub>t</sub>) log π(a<sub>t</sub>|s<sub>t</sub>)] − β 𝔼<sub>μ</sub>[ℍ(π(a<sub>t</sub>|s<sub>t</sub>))] + λ 𝔼<sub>μ</sub>[𝒫<sub>invalid</sub>(a<sub>t</sub>)],</p>
                        
            <p style="text-align: justify;">where <span style="font-family: math">ρ<sub>t</sub> = π(a<sub>t</sub>|s<sub>t</sub>)/μ(a<sub>t</sub>|s<sub>t</sub>)</span> is the importance sampling ratio between the target policy π and the behavior policy μ, ℍ is the entropy term for prevention of overfitting, 𝒫<sub>invalid</sub>(a<sub>t</sub>) imposes a penalty on actions deemed invalid based on task-specific criteria, β controls the strength of entropy regularization, and λ modulates the penalty's influence. The penalty is assigned using validation through pre-trained LLMs like Gemini, ensuring that inappropriate actions are penalized. This formulation encourages the agent to explore a diverse set of actions while constraining it to generate valid and meaningful commands, thereby enhancing both exploration and policy robustness when dealing with online non-stationarities.</p>
        </div>
        <img src="static/images/backbone.PNG" width="100%">
      </div>
    </div>
  </div>
</section>
<!-- End Plug-and-Play Agent Framework -->


<!-- Plug-and-Play Agent Framework -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">         
        <div class="column is-four-fifths">
          <div class="content">

            <h4>Off-policy correction: Retrace</h4>

            <p style="text-align: justify;">To enhance the estimation of the state-value function V(s<sub>t</sub>) in off-policy and asynchronous settings, we apply Retrace(λ) corrections directly to V(s<sub>t</sub>). The Retrace algorithm extends the <strong>TD</strong>(λ) method to off-policy learning by incorporating importance sampling ratios and a trace decay parameter λ. Specifically, we update V(s<sub>t</sub>) using the correction term δ<sub>t</sub>, computed as:</p>

            <p style="text-align: center; font-size: 1.25rem; font-family: math">V(s<sub>t</sub>) ← V(s<sub>t</sub>) + δ<sub>t</sub></p>

            <p style="text-align: center; font-size: 1.25rem; font-family: math">δ<sub>t</sub> = Σ<sub>k=t</sub><sup>H</sup> γ<sup>k-t</sup> (∏<sub>i=t+1</sub><sup>k</sup> c<sub>i</sub>) [r<sub>k</sub> + γV(s<sub>k+1</sub>) - V(s<sub>k</sub>)]</p>

            <p style="text-align: justify;">where c<sub>i</sub> = λ min(1, ρ<sub>i</sub>) with λ ∈ [0,1] being the trace decay parameter, and ρ<sub>i</sub> is the importance sampling ratio as mentioned before. This correction term effectively adjusts the value estimates using future rewards and importance sampling, enabling off-policy learning while mitigating variance due to importance weights. By applying Retrace(λ), we improve the estimation of V(s<sub>t</sub>) in off-policy settings, enhancing the stability and convergence of the value network.</p>

            <h4>Distributed Prioritized Experience Replay (DPER)</h4>

            <p style="text-align: justify;">To improve sample efficiency, we employ <strong>Distributed Prioritized Experience Replay (DPER)</strong>. For each trajectory τ = {(s<sub>t</sub>, a<sub>t</sub>, r<sub>t</sub>, s<sub>t+1</sub>)}<sub>t=0</sub><sup>H</sup>, we compute the priority p(τ) as:</p>

            <p style="text-align: center; font-size: 1.25rem; font-family: math">p(τ) = w<sub>1</sub>|δ̄| + w<sub>2</sub>ρ̄ + w<sub>3</sub>ℍ̄</p>

            <p style="text-align: justify;">where |δ̄| is the average absolute temporal-difference (TD) error over the trajectory, calculated as δ<sub>t</sub> = r<sub>t</sub> + γV(s<sub>t+1</sub>) - V(s<sub>t</sub>); ρ̄ is the average importance sampling ratio ρ<sub>t</sub>; and ℍ̄ is the average policy entropy, ℍ<sub>t</sub> = -log π(a<sub>t</sub>|s<sub>t</sub>), encouraging exploration by encouraging policy uncertainty, thus avoiding early convergence to suboptimal policies during training in dynamic environments. The weights w<sub>1</sub>, w<sub>2</sub>, and w<sub>3</sub> balance the contributions of each component, which is selected by grid-search (See Appendix: Hyperparameters). Trajectories with higher priorities are replayed more frequently, focusing learning on the most informative experiences. Priorities are periodically updated based on the latest policy, recalculating them to focus learning on the most informative experiences, ensuring continual adaptation to evolving behavior policies.</p>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- End Plug-and-Play Agent Framework -->
 

<!-- Results -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">Results</h2>
      <div class="columns is-centered">
        <div class="column is-four-fifths">


  <table id="results-table" border="1" cellpadding="5" cellspacing="0" style="width:100%; border-collapse:collapse;">
    <thead>
      <tr>
          <th rowspan="2" style="padding:5px;">Framework Type</th>
          <th rowspan="2" style="padding:5px;">Framework Name</th>
          <th colspan="2" style="padding:5px;">General</th>
          <th colspan="2" style="padding:5px;">Web Shopping</th>
      </tr>
      <tr>
          <th style="padding:5px;">Training</th>
          <th style="padding:5px;">Test</th>
          <th style="padding:5px;">Training</th>
          <th style="padding:5px;">Test</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td rowspan="2" style="padding:5px; vertical-align: middle;">Prompting</td>
          <td style="padding:5px;text-align: left">AppAgent + GPT-4v</td>
          <td style="padding:5px;">41.4</td>
          <td style="padding:5px;">43.0</td>
          <td style="padding:5px;">31.2</td>
          <td style="padding:5px;">35.2</td>
      </tr>
      <tr>
          <td style="padding:5px;">AppAgent + Gemini</td>
          <td style="padding:5px;">39.1</td>
          <td style="padding:5px;">45.3</td>
          <td style="padding:5px;">30.5</td>
          <td style="padding:5px;">32.0</td>
      </tr>
      <tr>
          <td rowspan="4" style="padding:5px;vertical-align: middle">Learning</td>
          <td style="padding:5px; text-align: left">AutoUI</td>
          <td style="padding:5px;">38.3</td>
          <td style="padding:5px;">40.6</td>
          <td style="padding:5px;">42.2</td>
          <td style="padding:5px;">44.5</td>
      </tr>
      <tr>
          <td style="padding:5px;">DigiRL (single, online)</td>
          <td style="padding:5px;">64.6 ± 1.5</td>
          <td style="padding:5px;">59.9 ± 2.1</td>
          <td style="padding:5px;">63.3 ± 1.5</td>
          <td style="padding:5px;">59.6 ± 3.1</td>
      </tr>
      <tr>
          <td style="padding:5px;">DigiRL (multi)</td>
          <td style="padding:5px;">67.7 ± 1.3</td>
          <td style="padding:5px;">61.2 ± 2.4</td>
          <td style="padding:5px;">64.5 ± 1.1</td>
          <td style="padding:5px;">59.9 ± 2.8</td>
      </tr>
      <tr>
          <td style="padding:5px; font-weight:bold; font-style:italic;">DistRL (Ours)</td>
          <td style="padding:5px; font-weight:bold;">75.5 ± 0.2</td>
          <td style="padding:5px; font-weight:bold;">73.2 ± 1.1</td>
          <td style="padding:5px; font-weight:bold;">69.8 ± 0.5</td>
          <td style="padding:5px; font-weight:bold;">68.5 ± 1.7</td>
      </tr>
  </tbody>
</table>
<h2 class="subtitle has-text-centered" style="font-size: 1rem; margin-top: 1rem;">
  Main comparisons regarding the success rate of different agents across various settings. Each experiment is repeated three times and the mean and standard deviation are reported. Results are evaluated with our autonomous evaluator with the 128 user instructions in the train and test set.
</h2>


  <div class="content">
    <h3 class="subtitle is-6" style="margin-top: 1rem;">For full results and more details, please refer to our <a href="https://arxiv.org/abs/2410.14803" target="_blank" style="color:#3273dc;">paper</a>.</h3>
</div>
</div>
</div>
</div>
</div>
</section>
<!--End Results -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{wang2024distrl,
  title={DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents},
  author={Wang, Taiyi and Wu, Zhihao and Liu, Jianheng and Hao, Jianye and Wang, Jun and Shao, Kun},
  journal={arXiv preprint arXiv:2410.14803},
  year={2024}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

<style>
  #results-table{
    background-color: #FFFFFF;
  }
  #results-table > thead > tr > th {
    padding: 0.5rem;
    text-align: center;
    vertical-align: middle;
  }
  #results-table > tbody > tr > td {
    padding: 0.5rem;
  }
  #results-table > tbody > tr > td:nth-child(n+2) {
    text-align: center;
  }
</style>