<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents">
  <meta property="og:title" content="DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents"/>
  <meta property="og:description" content="An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents"/>
  <meta property="og:url" content="https://ai-agents-2030.github.io/DistRL/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="DistRL, Asynchronous Distributed Reinforcement Learning, On-Device Control Agents">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DISTRL: AN ASYNCHRONOUS DISTRIBUTED REINFORCEMENT LEARNING FRAMEWORK FOR ON-DEVICE CONTROL AGENTS</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 2.4rem;">DISTRL: <br/>AN ASYNCHRONOUS DISTRIBUTED <br/>REINFORCEMENT LEARNING FRAMEWORK <br/>FOR ON-DEVICE CONTROL AGENTS</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Taiyi Wang<sup>1 2 * ‚Ä†</sup>,
              </span>
              <span class="author-block">
                Zhihao Wu<sup>3 *</sup>,
              </span>
              <span class="author-block">
                Jianheng Liu<sup>4</sup>,
              </span>
              <span class="author-block">
                Jianye Hao<sup>3</sup>,
              </span>
              <span class="author-block">
                Jun Wang<sup>4</sup>,
              </span>
              <span class="author-block">
                Kun Shao<sup>3 ‚Ä†</sup>
              </span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> University of Cambridge<br><sup>2</sup> Powersense Technology Limited<br><sup>3</sup> Huawei Noah's Ark Lab</span><br><sup>4</sup> University College London</span>
                    <span class="eql-cntrb"><small><br><sup>* </sup>Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>‚Ä† </sup>Corresponding authors: Taiyi.Wang@cl.cam.ac.uk, shaokun2@huawei.com</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2410.14803" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/DistRL-lab/distrl-open" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.14803" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
              <h3 class="subtitle is-5">Comparison between DistRL Agent and DigiRL Agent on Real Web Browsing Task:</h3>
              <div style="text-align: center">
                <video poster="" id="tree" autoplay controls muted loop height="100%" width="75%">
                  <!-- Your video here -->
                  <source src="static/videos/compare_demo_v4.mp4" type="video/mp4">
                </video>
              </div>
              <h3 class="subtitle is-5">Host-worker Running Loop Illustrations for DistRL Asynchronous Framework:</h3>
              <div style="text-align: center">
                <video poster="" id="tree" autoplay controls muted loop height="100%" width="75%" style="margin-top: 1rem">
                  <!-- Your video here -->
                  <source src="static/videos/async_demo_v5_compressed.mp4" type="video/mp4">
                </video>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            On-device control agents, especially on mobile devices, are responsible for operating mobile devices to fulfill users' requests, enabling seamless and intuitive interactions. Integrating Multimodal Large Language Models (MLLMs) into these agents enhances their ability to understand and execute complex commands, thereby improving user experience. However, fine-tuning MLLMs for on-device control presents significant challenges due to limited data availability and inefficient online training processes. This paper introduces DistRL, a novel framework designed to enhance the efficiency of online RL fine-tuning for mobile device control agents. DistRL employs centralized training and decentralized data acquisition to ensure efficient fine-tuning in the context of dynamic online interactions. Additionally, the framework is backed by our tailor-made RL algorithm, which effectively balances exploration with the prioritized utilization of collected data to ensure stable and robust training. Our experiments show that, on average, DistRL delivers a 3X improvement in training efficiency and enables training data collection 2.4X faster than the leading synchronous multi-machine methods. Notably, after training, DistRL achieves a 20% relative improvement in success rate compared to state-of-the-art methods on general Android tasks from an open benchmark, significantly outperforming existing approaches while maintaining the same training time. These results validate DistRL as a scalable and efficient solution, offering substantial improvements in both training efficiency and agent performance for real-world, in-the-wild device control tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
 
<!-- System Design -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">System Design</h2>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">            
            <b>DistRL</b> is an asynchronous distributed reinforcement learning framework that decouples trajectory collection from policy learning. The framework consists of a central <b>Host Learner</b> equipped with GPUs for policy training, and multiple heterogeneous <b>Workers</b> running Android emulators or connected to mobile devices. This separation optimizes efficiency by aligning tasks with appropriate hardware - <b>CPU-intensive</b> environment interactions on worker machines and <b>GPU-accelerated</b> policy training on the host server. The host maintains a <b>Circular Replay Buffer</b> for storing trajectories and uses a <b>FIFO Queue</b> to manage incoming experiences from workers efficiently.
          </div>          
          <img src="static/images/system-design.PNG" width="100%">
        
          <h2 class="subtitle has-text-centered" style="font-size: 1rem; margin-top: 1rem;">
            The workers operate in parallel through multi-threading, with each thread executing the latest policy received from the <b>Host Learner</b>. Workers collect trajectories during environment interactions and asynchronously send them back to the host for training. The framework demonstrates excellent scalability - with <b>two 96 vCPU machines</b> supporting up to <b>32 concurrent emulators</b> and nearly linear scaling with worker performance.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End System Design -->
 
<!-- A-RIDE -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">A-RIDE: The Backbone of DistRL</h2>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <p>Our method employs advantage-based estimations to refine policy gradient updates, as an extension of Generalized Advantage Estimation (GAE) by Schulman et al. (2015), effectively balancing exploration and exploitation in the learning process.</p>
            <p>By introducing a trace decay parameter, A-RIDE manages the bias-variance trade-off in advantage calculations, optimizing the stability and convergence of the policy. A-RIDE incorporates enhancements tailored to distributed, asynchronous environments, ensuring robust policy stability and efficient learning in complex device control tasks.</p>
            <p>The <strong>Retrace(<span style="font-family: math">Œª</span>)</strong> update is defined as:</p>
            <p><span style="font-family: math">Q(s<sub>t</sub>, a<sub>t</sub>) ‚Üê Q(s<sub>t</sub>, a<sub>t</sub>) + Œ¥<sub>t</sub></span>, where the correction term Œ¥<sub>t</sub> is calculated as:</p>
            <p style="text-align :center; font-size: 1.25rem; font-family: math">Œ¥<sub>t</sub> = ‚àë<span style="position: relative"><sub>k=t</sub><sup style="position: absolute; left: 0">H</sup></span> Œ≥<sup>k‚àít</sup> (‚àè<span style="position: relative"><sub>i=t+1</sub><sup style="position: absolute; left: 0">k</sup></span> c<sub>i</sub>) [r<sub>k</sub> + Œ≥ Q(s<sub>k+1</sub>, a<sub>k+1</sub>) ‚àí Q(s<sub>k</sub>, a<sub>k</sub>)].</p>
            <p>Here, <span style="font-family: math">Q(s<sub>t</sub>, a<sub>t</sub>)</span> is the estimated action-value function; Œ≥ ‚àà [0, 1] is the discount factor; H is the time horizon; <span style="font-family: math">c<sub>i</sub> = Œª min(1, œÅ<sub>i</sub>)</span> with <span style="font-family: math">Œª ‚àà [0, 1]</span> being the trace decay parameter; <span style="font-family: math">œÅ<sub>i</sub> = œÄ(a<sub>i</sub>|s<sub>i</sub>) / Œº(a<sub>i</sub>|s<sub>i</sub>)</span> is the importance sampling ratio between the target policy œÄ and the behavior policy Œº.</p>
            <p>To ensure effective exploration within the action space and prevent the generation of nonsensical or invalid commands, we incorporate entropy regularization into the actor loss function:</p>
            <p style="text-align :center; font-size: 1.25rem; font-family: math">‚Ñí = ‚àíùîº<sub>Œº</sub>[œÅ<sub>t</sub> A(s<sub>t</sub>, a<sub>t</sub>) log œÄ(a<sub>t</sub>|s<sub>t</sub>)] ‚àí Œ≤ ùîº<sub>Œº</sub>[‚Ñç(œÄ(a<sub>t</sub>|s<sub>t</sub>))] + Œª ùîº<sub>Œº</sub>[ùí´<sub>invalid</sub>(a<sub>t</sub>)],</p>
            <p>where <span style="font-family: math">A(s<sub>t</sub>, a<sub>t</sub>)</span> represents the advantage function, defined as the difference between the action-value function <span style="font-family: math">Q(s<sub>t</sub>, a<sub>t</sub>)</span> and the state-value function <span style="font-family: math">V(s<sub>t</sub>)</span>: <span style="font-family: math">A(s<sub>t</sub>, a<sub>t</sub>) = Q(s<sub>t</sub>, a<sub>t</sub>) ‚àí V(s<sub>t</sub>)</span>. The entropy term ‚Ñç promotes exploration, and ùí´<sub>invalid</sub>(a<sub>t</sub>) imposes penalties on invalid actions. The parameters Œ≤ and Œª are tuned to balance exploration and stability.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End A-RIDE -->


<!-- DistRL Pipeline -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">DistRL Pipeline</h2>
      <div class="columns is-centered">         
        <div class="column is-four-fifths">
          <div class="content">
            <p><strong>DistRL</strong> adopts a distributed asynchronous setup where multiple worker agents generate trajectories under the behavior policy <strong>Œº</strong> and send them to a central learner.</p>
            <p>The trajectory reward is computed using the Monte Carlo estimate:</p>
            <p style="text-align :center; font-size: 1.25rem; font-family: math">L(V<sub>traj</sub>) = ‚àíùîº<sub>ŒΩ</sub>[r(s<sub>H</sub>, a<sub>H</sub>) log V<sub>traj</sub>(s<sub>H</sub>, a<sub>H</sub>) + (1 ‚àí r(s<sub>H</sub>, a<sub>H</sub>)) log (1 ‚àí V<sub>traj</sub>(s<sub>H</sub>, a<sub>H</sub>))].</p>
            <p>The actor is updated using policy gradients based on advantage estimates, and enhanced <strong>Retrace</strong> corrections are applied for off-policy learning. This process is distributed asynchronously across worker nodes, ensuring efficient fine-tuning in environments with sparse rewards and distributed delays.</p>
          </div>
        <img src="static/images/backbone.PNG" width="100%">
      </div>
    </div>
  </div>
</section>
<!-- End DistRL Pipeline -->
 

<!-- Results -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">Results</h2>
      <div class="columns is-centered">
        <div class="column is-three-fifths">
          <img src="static/images/results-plot.PNG" width="100%">
          <h2 class="subtitle has-text-centered" style="font-size: 1rem; margin-top: 1rem;">
            Training performance (32 emulators) between the current SoTA (DigiRL) and DistRL, highlighting the enhanced efficiency of DistRL‚Äôs distributed framework during online training. (a) Wall-clock time comparison; (b) Training efficiency comparison; (c) Accumulated trajectories collection ability comparison; (d) Scalability of different training frameworks
          </h2>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <table id="results-table" border="1" cellpadding="5" cellspacing="0" style="width:100%; border-collapse:collapse;">
              <thead>
                <tr>
                    <th rowspan="2" style="padding:5px;">Framework Type</th>
                    <th rowspan="2" style="padding:5px;">Framework Name</th>
                    <th colspan="2" style="padding:5px;">General</th>
                    <th colspan="2" style="padding:5px;">Web Shopping</th>
                </tr>
                <tr>
                    <th style="padding:5px;">Training</th>
                    <th style="padding:5px;">Test</th>
                    <th style="padding:5px;">Training</th>
                    <th style="padding:5px;">Test</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td rowspan="2" style="padding:5px; vertical-align: middle;">Prompting</td>
                    <td style="padding:5px;text-align: left">AppAgent + GPT-4v</td>
                    <td style="padding:5px;">41.4</td>
                    <td style="padding:5px;">43.0</td>
                    <td style="padding:5px;">31.2</td>
                    <td style="padding:5px;">35.2</td>
                </tr>
                <tr>
                    <td style="padding:5px;">AppAgent + Gemini</td>
                    <td style="padding:5px;">39.1</td>
                    <td style="padding:5px;">45.3</td>
                    <td style="padding:5px;">30.5</td>
                    <td style="padding:5px;">32.0</td>
                </tr>
                <tr>
                    <td rowspan="4" style="padding:5px;vertical-align: middle">Learning</td>
                    <td style="padding:5px; text-align: left">AutoUI</td>
                    <td style="padding:5px;">38.3</td>
                    <td style="padding:5px;">40.6</td>
                    <td style="padding:5px;">42.2</td>
                    <td style="padding:5px;">44.5</td>
                </tr>
                <tr>
                    <td style="padding:5px;">DigiRL (single, online)</td>
                    <td style="padding:5px;">64.6 ¬± 1.5</td>
                    <td style="padding:5px;">59.9 ¬± 2.1</td>
                    <td style="padding:5px;">63.3 ¬± 1.5</td>
                    <td style="padding:5px;">59.6 ¬± 3.1</td>
                </tr>
                <tr>
                    <td style="padding:5px;">DigiRL (multi)</td>
                    <td style="padding:5px;">67.7 ¬± 1.3</td>
                    <td style="padding:5px;">61.2 ¬± 2.4</td>
                    <td style="padding:5px;">64.5 ¬± 1.1</td>
                    <td style="padding:5px;">59.9 ¬± 2.8</td>
                </tr>
                <tr>
                    <td style="padding:5px; font-weight:bold; font-style:italic;">DistRL (Ours)</td>
                    <td style="padding:5px; font-weight:bold;">75.5 ¬± 0.2</td>
                    <td style="padding:5px; font-weight:bold;">73.2 ¬± 1.1</td>
                    <td style="padding:5px; font-weight:bold;">69.8 ¬± 0.5</td>
                    <td style="padding:5px; font-weight:bold;">68.5 ¬± 1.7</td>
                </tr>
            </tbody>
          </table>
          <h2 class="subtitle has-text-centered" style="font-size: 1rem; margin-top: 1rem;">
            Main comparisons regarding the success rate of different agents across various settings. Each experiment is repeated three times and the mean and standard deviation are reported. Results are evaluated with our autonomous evaluator with the 128 user instructions in the train and test set.
          </h2>
          <div class="content">
            <h3 class="subtitle is-6" style="margin-top: 1rem;">For full results and more details, please refer to our <a href="https://arxiv.org/abs/2410.14803" target="_blank" style="color:#3273dc;">paper</a>.</h3>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>
<!--End Results -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{wang2024distrl,
  title={DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents},
  author={Wang, Taiyi and Wu, Zhihao and Liu, Jianheng and Hao, Jianye and Wang, Jun and Shao, Kun},
  journal={arXiv preprint arXiv:2410.14803},
  year={2024}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

<style>
  #results-table{
    background-color: #FFFFFF;
  }
  #results-table > thead > tr > th {
    padding: 0.5rem;
    text-align: center;
    vertical-align: middle;
  }
  #results-table > tbody > tr > td {
    padding: 0.5rem;
  }
  #results-table > tbody > tr > td:nth-child(n+2) {
    text-align: center;
  }
  /* Add these new mobile-responsive rules */
  @media screen and (max-width: 768px) {
    #results-table {
      font-size: 0.75rem;  /* Reduce font size on mobile */
    }
    
    #results-table > thead > tr > th,
    #results-table > tbody > tr > td {
      padding: 0.2rem;  /* Reduce padding on mobile */
    }

    /* Create horizontal scroll for table on mobile */
    .table-container {
      width: 100%;
      overflow-x: auto;
      -webkit-overflow-scrolling: touch;
    }

    .column > .content > div > video {
      width: 100%;
    }
  }
</style>